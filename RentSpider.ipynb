{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "url = 'https://www.stproperty.sg'\n",
    "driver=webdriver.Chrome()           \n",
    "driver.get(\"https://www.stproperty.sg/property-for-rent/max-rent-price-50000/max-floor-10000/min-bedroom-0/max-bedroom-10/max-psf-5000/from-sg/page1/size-10/sort-date-desc\")    \n",
    "time.sleep(5)                         \n",
    "\n",
    "urllist = []\n",
    "\n",
    "data = driver.page_source\n",
    "soup = BeautifulSoup(data, 'lxml')\n",
    "jobs = soup.findAll('a',{'class':'list-container-pic'})   \n",
    "for job in jobs:\n",
    "    urllist.append(url+job['href'])  \n",
    "for i in range(1,160): # 1600 items\n",
    "    driver.find_element_by_link_text('Next').click()\n",
    "    time.sleep(5)\n",
    "    data = driver.page_source\n",
    "    soup = BeautifulSoup(data, 'lxml')\n",
    "    jobs = soup.findAll('a',{'class':'list-container-pic'})     \n",
    "    for job in jobs:\n",
    "        urllist.append(url+job['href'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlwt\n",
    "f = xlwt.Workbook()\n",
    "sheet1 = f.add_sheet(u'sheet1',cell_overwrite_ok=True)\n",
    "for i in range(len(urllist)):\n",
    "    sheet1.write(i,0,urllist[i])\n",
    "f.save('url.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.stproperty.sg/condo-for-rent/condominium-for-rent/pavilion-11/37511152\n"
     ]
    }
   ],
   "source": [
    "import xlrd\n",
    "workbook=xlrd.open_workbook(\"url.xls\") \n",
    "worksheet=workbook.sheet_by_index(0)\n",
    "urllist=worksheet.col_values(0)\n",
    "print(urllist[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlwt\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "datalist = [['agent','title','street_name','post_code','area','bed_num','bath_num','posting_date','description','property_name','property_type',\n",
    "            'price','price_psf','tenure','built_in_area','detail','furnished','floor_location','built_year','lease_term','mrt_1','mrt_1_distance',\n",
    "            'mrt_2','mrt_2_distance','mrt_3','mrt_3_distance','school_1','school_1_distance','school_2','school_2_distance','school_3',\n",
    "            'school_3_distance','supermarket_1','supermarket_1_distance','supermarket_2','supermarket_2_distance','supermarket_3',\n",
    "            'supermarket_3_distance','childcare_1','childcare_1_distance','childcare_2','childcare_2_distance','childcare_3',\n",
    "            'childcare_3_distance','facilities_1','facilities_2','unit_information','lat','lng']]\n",
    "\n",
    "def getData(soup,static_soup):\n",
    "    agent = soup.select('div.col-xs-12.agent-section div.row div.col-xs-12 div.col-xs-7 strong')[0].string if soup.select('div.col-xs-12.agent-section div.row div.col-xs-12 div.col-xs-7 strong') else \"\"\n",
    "    title = soup.find('h4',{'class':'title-detail-page'}).string.strip() if soup.find('h4',{'class':'title-detail-page'}) else \"\"\n",
    "    street_name = soup.find('a',{'title':'Street Name'}).string if soup.find('a',{'title':'Street Name'}) else \"\"\n",
    "    post_code = str(soup.find('div',{'class':'page-header'}).contents[4].contents[2]).replace(',','').replace('<br/>','')\n",
    "    area = soup.find('div',{'class':'page-header'}).contents[4].contents[5].string\n",
    "    bed_num = soup.find('span',{'class':'icon-bed'}).string if soup.find('span',{'class':'icon-bed'}) else \"\"\n",
    "    bath_num = soup.find('span',{'class':'icon-bathroom'}).string if soup.find('span',{'class':'icon-bathroom'}) else \"\"\n",
    "    posting_date = str(soup.select('div.box-col640.pull-right.default-box-detail div.row div.col-xs-6 div.row div.col-xs-7')[-1])[22:-6]\n",
    "    description = str(soup.select('div.detail-tab-content p')[1]).replace('<p>','').replace('<br/>','').replace('</p>','')\n",
    "    property_name = soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-rent/search/')})[0].string if soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-rent/search/')}) else \"\"\n",
    "    property_type = soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-rent/\\w+-for-rent/[\\w-]+-for-rent$')})[0].string if soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-rent/\\w+-for-rent/[\\w-]+-for-rent$')}) else \"\"\n",
    "    price = soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-rent/\\w+-for-rent/min-rent-price')})[0].string if soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-rent/\\w+-for-rent/min-rent-price')}) else \"\"\n",
    "    price_psf = soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-rent/\\w+-for-rent/min-psf')})[0].string if soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-rent/\\w+-for-rent/min-psf')}) else \"\"\n",
    "    tenure = soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-rent/\\w+-for-rent/[\\w-]+-for-rent/')})[0].string if soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-rent/\\w+-for-rent/[\\w-]+-for-rent/')}) else \"\"\n",
    "    built_in_area = soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-rent/\\w+-for-rent/min-floor-')})[0].string if soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-rent/condo-for-rent/min-floor-')}) else \"\"\n",
    "    \n",
    "    detail = ''\n",
    "    detail_tag = soup.select('div.detail-tab-content div.row div.col-xs-6 div.row div.col-xs-6 span')\n",
    "    for index in range(len(detail_tag)):\n",
    "        detail = detail + detail_tag[index].string.strip() + '###'\n",
    "    furnished = detail_tag[-4].string.strip()\n",
    "    floor_location = detail_tag[-3].string.strip()\n",
    "    built_year = detail_tag[-2].string.strip()\n",
    "    lease_term = detail_tag[-1].string.strip()\n",
    "    \n",
    "    mrt_1 = soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/nearby-mrt-')})[0].string if soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/nearby-mrt-')}) else \"\"\n",
    "    mrt_1_distance = str(soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/nearby-mrt-')})[0].next_sibling)[2:-1] if soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/nearby-mrt-')}) else \"\"\n",
    "    mrt_2 = soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/nearby-mrt-')})[1].string if soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/nearby-mrt-')}) else \"\"\n",
    "    mrt_2_distance = str(soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/nearby-mrt-')})[1].next_sibling)[2:-1] if soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/nearby-mrt-')}) else \"\"\n",
    "    mrt_3 = soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/nearby-mrt-')})[2].string if soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/nearby-mrt-')}) else \"\"\n",
    "    mrt_3_distance = str(soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/nearby-mrt-')})[2].next_sibling)[2:-1] if soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/nearby-mrt-')}) else \"\"\n",
    "    school_1 = soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/near-school-')})[0].string if soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/near-school-')}) else \"\"\n",
    "    school_1_distance = str(soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/near-school-')})[0].next_sibling)[2:-1] if soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/near-school-')}) else \"\"\n",
    "    school_2 = soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/near-school-')})[1].string if soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/near-school-')}) else \"\"\n",
    "    school_2_distance = str(soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/near-school-')})[1].next_sibling)[2:-1] if soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/near-school-')}) else \"\"\n",
    "    school_3 = soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/near-school-')})[2].string if soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/near-school-')}) else \"\"\n",
    "    school_3_distance = str(soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/near-school-')})[2].next_sibling)[2:-1] if soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/near-school-')}) else \"\"\n",
    "    supermarket_1 = soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/near-mall-')})[0].string if soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/near-mall-')}) else \"\"\n",
    "    supermarket_1_distance = str(soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/near-mall-')})[0].next_sibling)[2:-1] if soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/near-mall-')}) else \"\"\n",
    "    supermarket_2 = soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/near-mall-')})[1].string if soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/near-mall-')}) else \"\"\n",
    "    supermarket_2_distance = str(soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/near-mall-')})[1].next_sibling)[2:-1] if soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/near-mall-')}) else \"\"\n",
    "    supermarket_3 = soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/near-mall-')})[2].string if soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/near-mall-')}) else \"\"\n",
    "    supermarket_3_distance = str(soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/near-mall-')})[2].next_sibling)[2:-1] if soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/near-mall-')}) else \"\"\n",
    "    childcare_1 = soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/near-childcare-')})[0].string if soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/near-childcare-')}) else \"\"\n",
    "    childcare_1_distance = str(soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/near-childcare-')})[0].next_sibling)[2:-1] if soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/near-childcare-')}) else \"\"\n",
    "    childcare_2 = soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/near-childcare-')})[1].string if soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/near-childcare-')}) else \"\"\n",
    "    childcare_2_distance = str(soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/near-childcare-')})[1].next_sibling)[2:-1] if soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/near-childcare-')}) else \"\"\n",
    "    childcare_3 = soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/near-childcare-')})[2].string if soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/near-childcare-')}) else \"\"\n",
    "    childcare_3_distance = str(soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/near-childcare-')})[2].next_sibling)[2:-1] if soup.findAll(name='a',attrs={\"href\":re.compile(r'^/property-for-sale/near-childcare-')}) else \"\"\n",
    "    facilities_1 = str(soup.select('div.detail-tab-content div.row div.col-xs-12 div.row')[0]).replace('<div class=\"row\">','').replace('<div class=\"col-xs-3\">',',').replace('</div>','').strip() if soup.select('div.detail-tab-content div.row div.col-xs-12 div.row') else \"\"\n",
    "    facilities_2 = str(soup.find('div',{'id':'facilities'}).contents[1]).replace('<div class=\"detail-tab-content\">','').replace('<p>','').replace('</p>','').replace('<br/>','').replace('</div>','').replace('<hr/>','').replace('<strong>','').replace('</strong>',':').strip() if soup.find('div',{'id':'facilities'}) else \"\"\n",
    "    unit_information = str(soup.find('div',{'id':'unit-information'}).contents[1].contents[4]).strip() if soup.find('div',{'id':'unit-information'}) else \"\"\n",
    "\n",
    "    lat = ''\n",
    "    lng = ''\n",
    "    if static_soup.findAll(name='script',attrs={\"src\":re.compile(r'https://maps.googleapis.com/')}):\n",
    "        tag = static_soup.findAll(name='script',attrs={\"src\":re.compile(r'https://maps.googleapis.com/')})[0].next_siblings\n",
    "        coordinate = []\n",
    "        for item in tag:\n",
    "            coordinate.append(item)\n",
    "        gmap = str(coordinate[1])\n",
    "        lat = re.findall(r'var lat=(.*);',gmap)[0]\n",
    "        lng = re.findall(r'var lng=(.*);',gmap)[0]\n",
    "\n",
    "    return [agent,title,street_name,post_code,area,bed_num,bath_num,posting_date,description,property_name,property_type,\n",
    "            price,price_psf,tenure,built_in_area,detail,furnished,floor_location,built_year,lease_term,mrt_1,mrt_1_distance,\n",
    "            mrt_2,mrt_2_distance,mrt_3,mrt_3_distance,school_1,school_1_distance,school_2,school_2_distance,school_3,\n",
    "            school_3_distance,supermarket_1,supermarket_1_distance,supermarket_2,supermarket_2_distance,supermarket_3,\n",
    "            supermarket_3_distance,childcare_1,childcare_1_distance,childcare_2,childcare_2_distance,childcare_3,\n",
    "            childcare_3_distance,facilities_1,facilities_2,unit_information,lat,lng]\n",
    "\n",
    "\n",
    "driver=webdriver.Chrome() \n",
    "for url in urllist:\n",
    "    tempurl = re.split(r'/(\\d+)$',url)\n",
    "    static_url = tempurl[0]+'/view-location/'+tempurl[1]\n",
    "    response  = requests.get(static_url)\n",
    "    static_soup = BeautifulSoup(response.content, 'lxml')\n",
    "    driver.get(url)\n",
    "    time.sleep(4)\n",
    "    data = driver.page_source\n",
    "    soup = BeautifulSoup(data, 'lxml')\n",
    "    row = getData(soup,static_soup)\n",
    "    datalist.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport xlwt\\nf = xlwt.Workbook()\\nsheet1 = f.add_sheet(u'sheet1',cell_overwrite_ok=True)\\nfor i in range(len(datalist)):\\n    for j in range(len(datalist[0])):\\n        data = str(datalist[i][j])\\n        sheet1.write(i,j,data)\\nf.save('data1.xls')\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xlwt\n",
    "f = xlwt.Workbook()\n",
    "sheet1 = f.add_sheet(u'sheet1',cell_overwrite_ok=True)\n",
    "for i in range(len(datalist)):\n",
    "    for j in range(len(datalist[0])):\n",
    "        data = str(datalist[i][j])\n",
    "        sheet1.write(i,j,data)\n",
    "f.save('data.xls')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
